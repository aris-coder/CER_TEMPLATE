\subsection{Définition des mots-clés}
\begin{enumerate}[nosep]
    \item \textbf{Carte perforée :} Support de stockage de données utilisé dans les premiers ordinateurs, constitué de cartes avec des trous représentant des informations.
    \item \textbf{Informatique de société :} Étude de l’impact de l’informatique sur les organisations, la vie quotidienne et les relations sociales.
    \item \textbf{\LaTeX{} :} Système de composition de documents, particulièrement utilisé pour les textes scientifiques et techniques, permettant un formatage précis.
    \item \textbf{ENIAC :} Premier ordinateur électronique à usage général, développé dans les années 1940, capable de réaliser des calculs complexes rapidement.
    \item \textbf{Base de données centralisée :} Structure où toutes les informations sont stockées dans un emplacement unique, permettant un accès et une gestion centralisés.
    \item \textbf{Cycle en V, Agile, DevOps :} Méthodologies de développement logiciel visant à structurer les phases de conception, de développement et de déploiement.
    \item \textbf{Licence libre :} Licence permettant d’utiliser, modifier et redistribuer un logiciel sans restrictions majeures.
    \item \textbf{GNU :} Projet de logiciel libre initié par Richard Stallman, visant à créer un système d’exploitation entièrement libre.
    \item \textbf{Pascal :} Langage de programmation conçu pour l’enseignement et le développement structuré de logiciels.
    \item \textbf{Éthique :} Ensemble de principes régissant le comportement humain, notamment dans le domaine des technologies et de l’informatique.
    \item \textbf{IA (Intelligence Artificielle) :} Discipline informatique visant à créer des systèmes capables de simuler l’intelligence humaine.
    \item \textbf{Babbage :} Charles Babbage, mathématicien considéré comme le “père de l’ordinateur” pour ses travaux sur la machine analytique.
    \item \textbf{Cloud :} Infrastructure informatique permettant de stocker, traiter et accéder à des données via Internet.
    \item \textbf{Licence propriétaire :} Licence qui impose des restrictions sur l’utilisation, la modification et la redistribution d’un logiciel.
\end{enumerate}

\subsection{Études de l’histoire de l’informatique}
L’histoire de l’informatique retrace la transformation des méthodes de calcul et du traitement de l’information, depuis les dispositifs mécaniques (comme les machines à calculer et les cartes perforées) jusqu’aux systèmes numériques modernes. On y distingue plusieurs étapes clés : les idées précurseures (Babbage, la machine analytique), l’ère des calculateurs électromécaniques et des cartes perforées, l’avènement des ordinateurs électroniques (ENIAC, etc.), l’apparition des langages de programmation de haut niveau (Fortran, Pascal, C), puis l’émergence des architectures distribuées, du Web et des services cloud. Chaque période s’accompagne d’évolutions techniques (miniaturisation, augmentation de la puissance de calcul), organisationnelles (développement industriel, normalisation) et sociales (accès à l’information, transformation du travail).

Étudier cette histoire permet de comprendre pourquoi certains choix technologiques perdurent (par héritage, compatibilité), d’identifier les ruptures conceptuelles (programmation structurée, orientée objet, calcul distribué) et d’évaluer les conséquences non techniques (régulation, vie privée, inégalités d’accès). Pour un travail académique, il est utile de structurer l’étude en : 1) chronologie synthétique, 2) portraits des inventions et inventeurs majeurs, 3) impacts sociaux et économiques à chaque période, 4) leçons techniques (par ex. modularité, abstractions) et 5) perspectives futures.

\subsection{Études sur les outils numériques (cloud, IA) de navigation}
Les outils numériques contemporains pour la « navigation » (ici entendue comme guidage, localisation, routage, cartographie et services associés) reposent principalement sur deux piliers : l’infrastructure Cloud et les techniques d’Intelligence Artificielle. Le cloud offre la puissance de calcul, le stockage cartographique, la distribution en temps réel (tuiles cartographiques, géocodage, API de routage) et les services scalables pour gérer des milliers d’utilisateurs simultanés. L’IA intervient dans l’optimisation des trajets (algorithmes d’optimisation et d’apprentissage), la reconnaissance d’images (pour la cartographie à partir de photos/aériennes), la fusion de capteurs (GPS, IMU, lidars) et la prédiction du trafic.

L’étude technique doit couvrir : architectures cloud (IaaS, PaaS, services managés), solutions de stockage cartographique (base spatiale, GeoJSON/PostGIS), APIs de navigation et leurs limites (précision, latence), ainsi que méthodes d’IA appliquées (apprentissage supervisé pour détection d’obstacles, apprentissage par renforcement pour planification de trajectoire). Sur le plan pratique, comparer les fournisseurs cloud, les coûts, les contraintes de latence et la confidentialité des flux de localisation est essentiel. Enfin, il faut prévoir la robustesse (déconnexion, dégradation de service) et la sécurité (authentification, chiffrement des traces).

\subsection{Études sur les impacts éthiques et sociétaux}
Les technologies informatiques modifient profondément les relations sociales, le travail, la vie politique et la vie privée. Les impacts éthiques incluent la surveillance (collecte massive de données de localisation), les biais algorithmiques (décisions automatisées favorisant ou discriminant certains groupes), l’opacité des systèmes (boîtes noires) et la responsabilité (qui est responsable en cas d’erreur d’un système autonome ?). Les enjeux sociétaux touchent l’emploi (automatisation), l’accès aux services (fracture numérique), la concentration de pouvoir (grands acteurs du numérique) et la gouvernance des données.

Une étude complète doit intégrer : 1) cartographie des parties prenantes (utilisateurs, entreprises, États, ONG), 2) risques et bénéfices (sécurité publique vs vie privée), 3) cadres réglementaires existants (RGPD, lois nationales), 4) bonnes pratiques éthiques (privacy by design, audits algorithmiques, transparence) et 5) recommandations opérationnelles (minimisation des données, consentement éclairé, mécanismes de recours). L’approche doit être interdisciplinaire, mêlant informatique, droit, sociologie et philosophie morale.

\subsection{Études sur l’importance des licences}
Les licences logicielles déterminent les droits d’utilisation, de modification et de redistribution. Elles influencent la réutilisabilité, la collaboration, le modèle économique et la pérennité des projets. Les deux grandes familles sont : les licences libres (ex. GPL, MIT, Apache) qui permettent partage et modification selon des conditions variables, et les licences propriétaires qui restreignent l’accès et l’usage. Comprendre ces licences est crucial pour choisir une stratégie de développement (open source pour adoption et auditabilité ; propriétaire pour monétisation contrôlée).

L’étude doit aborder : typologie des licences et leurs obligations (copyleft vs permissive), conséquences juridiques (compatibilités, exigences de distribution), impacts pratiques (contribution externe, sécurité par la transparence) et recommandations pour les projets académiques ou industriels (quelle licence selon objectifs : diffusion, contrôle, partenariats). Il est aussi utile d’examiner les implications pour la dépendance aux fournisseurs (vendor lock-in) et pour la conformité des composants tiers (scan de licence).

\subsection{Études sur la méthodologie de développement}
Les méthodologies structurent le processus de conception, développement, test et maintenance du logiciel. Parmi les plus répandues : le cycle en V (orientation documentation et validation), Agile (itérations courtes, feedback utilisateur) et DevOps (intégration continue, déploiement continu, culture d’automatisation). Chaque méthodologie présente des forces : le cycle en V favorise la traçabilité dans les projets critiques, Agile améliore l’adaptabilité aux changements, DevOps accélère la mise en production et la qualité opérationnelle.

L’analyse doit comparer ces approches selon des critères : taille de l’équipe, criticité du logiciel, régulation, exigences de conformité et fréquence des livraisons. Proposer un processus hybride est souvent pertinent (par ex. design critique en cycle en V pour la partie hardware embarqué + développement logiciel en Agile/DevOps). Inclure les bonnes pratiques : gestion de versions, revue de code, tests automatisés, CI/CD, gestion des incidents et documentation continue.

\subsection{Étude sur la communication technique}
La communication technique vise à rendre compréhensible l’information technique pour des publics variés (développeurs, décideurs, utilisateurs finaux). Elle inclut la rédaction de documentation (guides d’installation, API docs, manuels utilisateur), la production de schémas et diagrammes, et la transmission orale (présentations, formations). Une communication claire réduit les erreurs d’usage, facilite la maintenance et favorise l’adoption.

L’étude devrait proposer des standards (ex. architecture de la documentation, templates), des outils (Sphinx, Markdown/ReadTheDocs, diagrammes UML), et des méthodologies (documentation as code, revue documentaire intégrée au workflow). Traiter aussi l’accessibilité (langage simple, localisation), et la mise à jour continue de la documentation via automatisation (génération d’API docs, tests d’exemples). Enfin, recommander des indicateurs de qualité documentaire (couverture, actualité, retours utilisateurs).

\subsection{Synthèse}
L’informatique, en tant que science et pratique, ne se limite pas à l’écriture de programmes. Elle s’inscrit dans une évolution historique, s’appuie sur des outils numériques modernes, soulève des enjeux éthiques et sociétaux, se structure grâce à des méthodologies, repose sur des licences claires et exige une communication technique efficace. La compréhension de ces dimensions est indispensable pour toute approche scientifique ou professionnelle en informatique.
